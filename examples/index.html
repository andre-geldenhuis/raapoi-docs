<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://vuw-research-computing.github.io/raapoi-docs/examples/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Examples - Rāpoi Cluster Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="../extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Examples";
        var mkdocs_page_input_path = "examples.md";
        var mkdocs_page_url = "/raapoi-docs/examples/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Rāpoi Cluster Documentation
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Overview</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Documentation</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../accessing_the_cluster/">Accessing the Cluster</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basic_commands/">Basic Commands</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../storage/">Storage and Quotas</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../partitions/">Using Partitions</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../environment/">Preparing your Environment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../running_jobs/">Running Jobs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../parallel_processing/">Parallel Processing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../managing_jobs/">Managing Jobs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cloud_providers/">Connecting to Cloud Providers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../containers/">Using Containers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../notebooks/">Using Jupyter Notebooks</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Examples</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#simple-bash-example-start-here-if-new-to-hpc">Simple Bash Example - start here if new to HPC</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#simple-python-program-using-virtualenv-and-pip">Simple Python program using virtualenv and pip</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#using-anacondaminicondaconda-idba">Using Anaconda/Miniconda/conda - idba</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#loading-r-packages-running-a-simple-job">Loading R packages &amp; running a simple job</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#matlab-gpu-example">Matlab GPU example</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#job-arrays-running-many-similar-jobs">Job Arrays - running many similar jobs</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#simple-bash-job-array-example">Simple Bash Job Array example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#a-simple-r-job-array-example">A simple R job Array Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#singularity">Singularity</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#singularitydocker-container-example">Singularity/Docker container example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#singularitytensorflow-example">Singularity/TensorFlow Example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#singularitymaxbin2-example">Singularity/MaxBin2 Example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#singularitysandbox-example">Singularity/Sandbox Example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#singularitycustom-conda-container-idba-example">Singularity/Custom Conda Container - idba example</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../training/">Training</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../usersub/">User Submitted Docs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../support/">Support</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Rāpoi Cluster Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Documentation &raquo;</li><li>Examples</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="examples">Examples<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h1>
<h2 id="simple-bash-example-start-here-if-new-to-hpc">Simple Bash Example - start here if new to HPC<a class="headerlink" href="#simple-bash-example-start-here-if-new-to-hpc" title="Permanent link">&para;</a></h2>
<p>In this example we will run a very simple bash script on the quicktest partition.  The bash script is very simple, it just prints the hostname - the node you're running on - and prints the date into a file.  It also sleeps for 1 minute - it just does this to give you a chance to see your job in the queue with <code>squeue</code></p>
<p>First lets create a sensible working directory</p>
<pre class="codehilite"><code class="language-bash">mkdir bash_example
cd bash_example
</code></pre>

<p>We'll use the text editor nano to create our bash script as well as our submission script.  In real life, you might find it easier to create your code and submission script on your local machine, then copy them over as nano is not a great editor for large projects.</p>
<p>Create and edit our simple bash script - this is our code we will run on the HPC</p>
<pre class="codehilite"><code class="language-bash">nano test.sh
</code></pre>

<p>Paste or type the following into the file</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

hostname  #prints the host name to the terminal
date &gt; date_when_job_ran.txt  #puts the content of the date command into a txt file
sleep 1m # do nothing for 1 minute.  Job will still be &quot;running&quot; 
</code></pre>

<p>press ctrl-O to save the text in nano, then ctrl-X to exit nano.</p>
<p>Using nano again create a file called submit.sh with the following content</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash
#
#SBATCH --job-name=bash_test
#SBATCH -o bash_test.out
#SBATCH -e bash_test.err
#
#SBATCH --partition=quicktest
#
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --time=10:00

bash test.sh  #actually run our bash script, using bash
</code></pre>

<p>If you're familiar with bash scripts, the above is a bit weird.  The <code>#SBATCH</code> lines would normally be comments and hence not do anything, but Slurm will read those lines to determine how many resources to provide your job.  In this case we ask for the following:</p>
<ul>
<li>quicktest partition (the default - so you don't technically need to ask for it). </li>
<li>1 cpu per task - we have one task, so we're asking for 1 cpu</li>
<li>1 gig of memory.</li>
<li>a max runtime of 10 min</li>
</ul>
<p>If your job uses more memory or time than requested, Slurm will immediately kill it.  If you use more CPU's than requested - your job will keep running, but your "cpus" will be shared bewteen the CPUs you actually requested. So if your job tried to use 10 CPUs but you only asked for one, it'll run extremely slowly - don't do this.</p>
<p>Our <code>submit.sh</code> script also names our job <code>bash_test</code> this is what the job will show up as in squeue. We ask for things printed out on the terminal to go to two seperate files.  Normal, non error, things that would be printed out on the terminal will be put into the text file <code>bash_test.out</code>.  Errors will be printed into the text file <code>bash_test.err</code></p>
<p>Now submit your job to the Slurm queue.</p>
<pre class="codehilite"><code class="language-bash">sbatch submit.sh  

#See your job in the queue
squeue -u &lt;your_username&gt;

#When job is done see the new files
ls

#look at the content that would have been printed to the terminal if running locally
cat bash_test.out

# See the content of the file that your bash script created
cat date_when_job_ran.txt
</code></pre>

<h2 id="simple-python-program-using-virtualenv-and-pip">Simple Python program using virtualenv and pip<a class="headerlink" href="#simple-python-program-using-virtualenv-and-pip" title="Permanent link">&para;</a></h2>
<p>First we need to create a working directory and move there</p>
<pre class="codehilite"><code class="language-bash">mkdir python_test
cd python_test
</code></pre>

<p>Next we load the python 3 module and use python 3 to create a python virtualenv.  This way we can install pip packages which are not installed on the cluster</p>
<pre class="codehilite"><code class="language-bash">module load python/3.6.6
python3 -m venv mytest
</code></pre>

<p>Activate the <code>mytest</code> virtualenv and use pip to install the <code>webcolors</code> package</p>
<pre class="codehilite"><code class="language-bash">source mytest/bin/activate
pip install webcolors
</code></pre>

<p>Create the file test.py with the following contents using nano</p>
<pre class="codehilite"><code class="language-python">import webcolors
from random import randint
from socket import gethostname

colour_list = list(webcolors.CSS3_HEX_TO_NAMES.items())
requested_colour = randint(0,len(colour_list))
colour_name = colour_list[requested_colour][1]

print(&quot;Random colour name:&quot;, colour_name, &quot; on host: &quot;, gethostname())
</code></pre>

<p>Alternatively download it with wget:</p>
<pre class="codehilite"><code class="language-bash">wget https://raw.githubusercontent.com/\
    vuw-research-computing/raapoi-tools/\
    master/examples/python_venv/test.py
</code></pre>

<p>Using nano create the submissions script called python_submit.sh with the following content - change <code>me@email.com</code> to your email address.</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash
#
#SBATCH --job-name=python_test
#SBATCH -o python_test.out
#SBATCH -e python_test.err
#
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=1G
#SBATCH --time=10:00
#
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=me@email.com

module load python/3.6.6

source mytest/bin/activate
python test.py
</code></pre>

<p>Alternatively download it with wget</p>
<pre class="codehilite"><code class="language-bash">wget https://raw.githubusercontent.com/\
    vuw-research-computing/raapoi-tools/\
    master/examples/python_venv/python_submit.sh
</code></pre>

<p>To submit your job to the Slurm scheduler</p>
<pre class="codehilite"><code class="language-bash">sbatch python_submit.sh
</code></pre>

<p>Check for your job on the queue with <code>squeue</code> though it might finish very fast.  The output files will appear in your working directory.</p>
<!-- BEGIN INCLUDE examples/anaconda.md   -->
<h2 id="using-anacondaminicondaconda-idba">Using Anaconda/Miniconda/conda - idba<a class="headerlink" href="#using-anacondaminicondaconda-idba" title="Permanent link">&para;</a></h2>
<p>Many users use Anaconda/Miniconda to manage software stacks.  One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file.</p>
<p>However, this is also an additional bit of complexity so many users just use conda outside of singularity.  You can install your own version of Anaconda/Miniconda to your home directory or scratch.  We have also got packaged versions of Anaconda/Miniconda installed with our module loading system.</p>
<p>Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup.</p>
<pre class="codehilite"><code class="language-bash">module load old-mod-system/Anaconda3/2020.11 
</code></pre>

<p>Let's create a new conda environment for this example, in a sensible location, I used <code>~/examples/conda/idba</code></p>
<pre class="codehilite"><code class="language-bash">conda create --name idba-example  # press y for the Proceed prompt if it looks correct
conda activate idba-example  #activate our example environment.
</code></pre>

<p>Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba.</p>
<p>Install idba in our conda environment.</p>
<pre class="codehilite"><code class="language-bash">conda install -c bioconda idba
</code></pre>

<p>Idba is a genome assembler, we will use paired-end illumina reads of E. coli.  The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget.</p>
<pre class="codehilite"><code class="language-bash">mkdir data  # put our data in a sensible location
cd data
wget --content-disposition goo.gl/JDJTaz #sequence data
wget --content-disposition goo.gl/tt9fsn #sequence data
cd ..  #back to our project directory
</code></pre>

<p>The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the Rāpoi login node as it is a fast task that doesn't need many resources.</p>
<pre class="codehilite"><code class="language-bash">fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa
</code></pre>

<p>To create our submission script we need to know the path to our conda enviroment. To get this:</p>
<pre class="codehilite"><code class="language-bash">conda env list
</code></pre>

<p>You'll need to find your <code>idba-example</code> environment, and next to it is the path you'll need for your submission script.  In my case:</p>
<pre class="codehilite"><code class="language-bash"># conda environments:
#
base                  *  /home/andre/anaconda3
idba-example          /home/andre/anaconda3/envs/idba-example  # We need this line, it'll be different for you!
</code></pre>

<p>Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use <code>vuw-job-report &lt;job-id&gt;</code></p>
<p><em>idba_submit.sh</em></p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=idba_test
#SBATCH -o _output.out
#SBATCH -e _output.err
#SBATCH --time=00:5:00
#SBATCH --partition=quicktest
#SBATCH --ntasks=12
#SBATCH --mem=3G

module load old-mod-system/Anaconda3/2020.11
eval &quot;$(conda shell.bash hook)&quot; # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ...
conda activate /home/andre/anaconda3/envs/idba-example  # We will need to activate our conda enviroment on the remote node
idba idba_ud -r data/read.fa -o output
</code></pre>

<p>To submit our job</p>
<pre class="codehilite"><code class="language-bash">sbatch idba_submit.sh
</code></pre>

<p>To see our job running or queuing</p>
<pre class="codehilite"><code class="language-bash">squeue -u $USER
</code></pre>

<p>This job will take a few minutes to run, generally less than 5.
When the job is done we can see the output in the output folder.  We can also see the std output and std err in the files <code>_output.out and _output.err</code>.  The quickest way to examine them is to <code>cat</code> the files when the run is done.</p>
<pre class="codehilite"><code class="language-bash">cat _output.out
</code></pre>

<!-- END INCLUDE -->

<h2 id="loading-r-packages-running-a-simple-job">Loading R packages &amp; running a simple job<a class="headerlink" href="#loading-r-packages-running-a-simple-job" title="Permanent link">&para;</a></h2>
<p>First login to Rāpoi and load the R and R/CRAN modules:</p>
<pre class="codehilite"><code class="language-bash">module load R/4.0.2
module load R/CRAN      
</code></pre>

<p>Then run R on the command line:</p>
<pre class="codehilite"><code class="language-bash">R
</code></pre>

<p>Test library existence:</p>
<pre class="codehilite"><code class="language-R">&gt; library(tidyverse)
</code></pre>

<p>This should load the package, and give some output like this:</p>
<pre class="codehilite"><code class="language-R">── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──
✔ ggplot2 3.3.2     ✔ purrr   0.3.4
✔ tibble  3.0.1     ✔ dplyr   1.0.0
✔ tidyr   1.1.0     ✔ stringr 1.4.0
✔ readr   1.3.1     ✔ forcats 0.5.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
</code></pre>

<p>(These conflicts are normal and can be ignored.)</p>
<p>To quit R, type:</p>
<pre class="codehilite"><code class="language-R">&gt; q()
</code></pre>

<p>Next create a bash submission script called <code>r_submit.sh</code> (or another name of your choice) using your preferred text editor, e.g. nano.</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash
#
#SBATCH --job-name=r_test
#SBATCH -o r_test.out
#SBATCH -e r_test.err
#
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=1G
#SBATCH --time=10:00
#

module load R/4.0.2
module load R/CRAN

Rscript mytest.R
</code></pre>

<p>Save this to the current working directory, and then create another file using your preferred text editor called <code>mytest.R</code> (or another name of your choice) containing the following R commands:</p>
<pre class="codehilite"><code class="language-R">library(tidyverse)

sprintf(&quot;Hello World!&quot;)
</code></pre>

<p>then run it with the previously written bash script:  </p>
<pre class="codehilite"><code class="language-bash">sbatch r_submit.sh 
</code></pre>

<p>This submits a task that should execute quickly and create files in the directory from which it was run.
Examine <code>r_test.out</code>. You can use an editor like nano, vi or emacs, or you can just <code>cat</code> or <code>less</code> the file to see its contents on the terminal. You should see:
<code>"Hello World"</code></p>
<h2 id="matlab-gpu-example">Matlab GPU example<a class="headerlink" href="#matlab-gpu-example" title="Permanent link">&para;</a></h2>
<p>Matlab has various built-in routines which are GPU accelerated.  We will run a simple speed comparison between cpu and gpu tasks. In a sensible location create a file called <code>matlab_gpu.m</code>  I used <code>~/examples/matlab/cuda/matlab_gpu.m</code>.</p>
<pre class="codehilite"><code class="language-matlab">% Set an array which will calculate the Eigenvalues of
A=rand(1000);

% Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it.
Agpu=gpuArray(A);
tic
B=eig(Agpu);
t1=toc

% Let's compare the time with CPU
tic
B=eig(A);
t2=toc
</code></pre>

<p>We will also need a Slurm submission script; we'll call this <code>matlab_gpu.sh</code>. Note that we will need to use the new Easybuild module files for our cuda libraries, so make sure to include the module use line <code>module use /home/software/tools/eb_modulefiles/all/Core</code></p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=matlab-gpu-example
#SBATCH --output=out-gpu-example.out
#SBATCH --error=out-gpu-example.err
#SBATCH --time=00:05:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --ntasks=2
#SBATCH --mem=60G

module use /home/software/tools/eb_modulefiles/all/Core
module load matlab/2021a
module load fosscuda/2020b

matlab -nodisplay -nosplash -nodesktop -r &quot;run('matlab_gpu.m');exit;&quot;
</code></pre>

<p>To submit this job to the Slurm queue <code>sbatch matlab_gpu.sh</code>.  This job will take a few minutes to run - this is mostly the Matlab startup time.
Examine the queue for your job <code>squeue -u $USER</code>.  When your job is done, inspect the output file.  You can use an editor like nano, vi or emacs, or you can just <code>cat</code> or <code>less</code> the file to see its contents on the terminal.</p>
<pre class="codehilite"><code class="language-bash">cat out-gpu-example.out
</code></pre>

<p>What do you notice about the output?  Surely GPUs should be faster than the CPU!  It takes time for the GPU to start processing your task, the CPU is able to start the task far more quickly.  So for short operations, the CPU can be faster than the GPU - remember to benchmark your code for optimal performance!  Just because you can use a GPU for your task doesn't mean it is necessarily faster!</p>
<p>To get a better idea of the advantage of the GPU let's increase the size of the array from <code>1000</code> to <code>10000</code></p>
<p><em>matlab_gpu.m</em></p>
<pre class="codehilite"><code class="language-Matlab">% Set an array which will calculate the Eigenvalues of
A=rand(10000);

% Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it.
Agpu=gpuArray(A);
tic
B=eig(Agpu);
t1=toc

% Let's compare the time with CPU
tic
B=eig(A);
t2=toc
</code></pre>

<p>To make things fairer for the CPU in this case, we will also allocate half the CPUs on the node to Matlab.  Half the CPUs, half the memory and half the GPUs, just to be fair.</p>
<p><em>matlab_gpu.sh</em></p>
<pre class="codehilite"><code>#!/bin/bash

#SBATCH --job-name=matlab-gpu-example
#SBATCH --output=out-gpu-example.out
#SBATCH --error=out-gpu-example.err
#SBATCH --time=00:05:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --ntasks=128
#SBATCH --mem=256G

module use /home/software/tools/eb_modulefiles/all/Core
module load matlab/2021a
module load fosscuda/2020b

matlab -nodisplay -nosplash -nodesktop -r &quot;run('matlab_gpu.m');exit;&quot;
</code></pre>

<p>The output in my case was:</p>
<pre class="codehilite"><code class="language-bash">                            &lt; M A T L A B (R) &gt;
                  Copyright 1984-2021 The MathWorks, Inc.
             R2021a Update 1 (9.10.0.1649659) 64-bit (glnxa64)
                               April 13, 2021


To get started, type doc.
For product information, visit www.mathworks.com.


t1 =

   62.0212


t2 =

  223.0818
</code></pre>

<p>So in thise case the GPU was considerably faster.  Matlab can do this a bit faster on the CPU if you give it <strong>fewer</strong> CPUs, the optimum appears to be around 20, but it still takes 177s.  Again, optimise your resource requests for your problem, less can sometimes be more, however the GPU easily wins  in this case.</p>
<h2 id="job-arrays-running-many-similar-jobs">Job Arrays - running many similar jobs<a class="headerlink" href="#job-arrays-running-many-similar-jobs" title="Permanent link">&para;</a></h2>
<p>Slurm makes it easy to run many jobs which are similar to each other.  This could be one piece of code running over many datasets in parallel or running a set of simulations with a different set of parameters for each run.</p>
<h3 id="simple-bash-job-array-example">Simple Bash Job Array example<a class="headerlink" href="#simple-bash-job-array-example" title="Permanent link">&para;</a></h3>
<p>The following code will run the submission script 16 times as resources become available (i.e. they will not neccesarily run at the same time).  It will just print out the Slurm array task ID and exit.</p>
<p><em>submit.sh:</em></p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=test_array
#SBATCH --output=out_array_%A_%a.out
#SBATCH --error=out_array_%A_%a.err
#SBATCH --array=1-16
#SBATCH --time=00:00:20
#SBATCH --partition=parallel
#SBATCH --ntasks=1
#SBATCH --mem=1G

# Print the task id.
echo &quot;My SLURM_ARRAY_TASK_ID: &quot; $SLURM_ARRAY_TASK_ID

# Add lines here to run your computations.
</code></pre>

<p>Run the example with the standard</p>
<pre class="codehilite"><code class="language-bash">sbatch submit.sh
</code></pre>

<h3 id="a-simple-r-job-array-example">A simple R job Array Example<a class="headerlink" href="#a-simple-r-job-array-example" title="Permanent link">&para;</a></h3>
<p>As a slightly more practical example the following will run an R script 5 times as resources become available.  The R script takes as an input the <code>$SLURM_ARRAY_TASK_ID</code> which then selects a parameter <code>alpha</code> out of a lookup table.</p>
<p>This is one way you could run simulations or similar with a set parameters defined in a lookuop table in your code.</p>
<p>To make outputs more tidy and to help organisation, instead of dumping all the outputs into the directory with our code and submission script, we will separate the outputs into directories.  Dataframes saved from R will be saved to the output/ directory, and all output which would otherwise be printed to the commnd line (stdout and stderr) will be saved to the stdout/ directory. Both of these directories will need to be created before running the script.</p>
<p><em>r_random_alpha.R:</em></p>
<pre class="codehilite"><code class="language-R"># get the arguments supplied to R.  
# trailingOnly = TRUE gets the user supplied
# arguments, and for now we will only get the
# first user supplied argument
args &lt;- commandArgs(trailingOnly = TRUE)
inputparam &lt;- args[1]

# a vector with all our parameters.
alpha_vec &lt;- c(2.5, 3.3, 5.1, 8.2, 10.9)
alpha &lt;- alpha_vec[as.integer(inputparam)]

# Generate a random number between 0 and alpha 
# store it in dataframe with the coresponding 
# alpha value
randomnum &lt;- runif(1, min=0, max=as.double(alpha))
df &lt;- data.frame(&quot;alpha&quot; = alpha, &quot;random_num&quot; = randomnum)

# Save the data frame to a file with the alpha value
# Note that the output/ folder will need to be 
# manually created first!
outputname &lt;- paste(&quot;output/&quot;, &quot;alpha_&quot;, alpha, &quot;.Rda&quot;, sep=&quot;&quot;)
save(df,file=outputname)
</code></pre>

<p>Next create the submision script. Which we will run on the parallel partition rather than quicktest.</p>
<p>r_submit.sh:</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash
#SBATCH --job-name=test_R_array
#SBATCH --output=stdout/array_%A_%a.out
#SBATCH --error=stdout/array_%A_%a.err
#SBATCH --array=1-5
#SBATCH --time=00:00:20
#SBATCH --partition=parallel
#SBATCH --ntasks=1
#SBATCH --mem=1G

module load R/CRAN

# Print the task id.
Rscript r_random_alpha.R $SLURM_ARRAY_TASK_ID
</code></pre>

<p>Run the jobs with</p>
<pre class="codehilite"><code class="language-bash">sbatch r_submit.sh
</code></pre>

<h2 id="singularity">Singularity<a class="headerlink" href="#singularity" title="Permanent link">&para;</a></h2>
<p>While there are many modules on Rāpoi, sometimes you might want to install your own packages in your own way.  Singularity allows you to do this.  If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the Rāpoi.  However, you <em>can</em> have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster.</p>
<h3 id="singularitydocker-container-example">Singularity/Docker container example<a class="headerlink" href="#singularitydocker-container-example" title="Permanent link">&para;</a></h3>
<p>Singularity allows you to use most (but not all!) docker images on Rāpoi.</p>
<p>On your local machine create the singularity definition file</p>
<p>input_args_example.def</p>
<pre class="codehilite"><code class="language-singularity">BootStrap: library
From: ubuntu:16.04

%runscript
    exec echo &quot;$@&quot;

%labels
    Author Andre
</code></pre>

<p>This will build an ubuntu 16.04 container that will eventually run on Rāpoi which runs Centos.  This container has a runscript which just echos back any arguments sent to the container when your start it up.</p>
<p>Build the container <em>locally</em> with sudo and singularity</p>
<pre class="codehilite"><code class="language-bash">sudo singularity build inputexample.sif input_args_example.def 
</code></pre>

<p>This will build an image that you can't modify any further and is immediately suitable to run on Rāpoi
Copy this file to Rāpoi via sftp</p>
<pre class="codehilite"><code class="language-bash">sftp &lt;username&gt;@raapoi.vuw.ac.nz
</code></pre>

<p>Create a submit script using singularity on the cluster</p>
<p>singularity_submit.sh</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=singularity_test
#SBATCH -o sing_test.out
#SBATCH -e sing_test.err
#SBATCH --time=00:00:20
#SBATCH --ntasks=1
#SBATCH --mem=1G

module load singularity

singularity run inputtest.sif &quot;hello from a container&quot;
</code></pre>

<p>Run the script with the usual</p>
<pre class="codehilite"><code class="language-bash">singularity_submit.sh
</code></pre>

<h3 id="singularitytensorflow-example">Singularity/TensorFlow Example<a class="headerlink" href="#singularitytensorflow-example" title="Permanent link">&para;</a></h3>
<p>tensor.def</p>
<pre class="codehilite"><code class="language-bash">Bootstrap: docker
From: tensorflow/tensorflow:latest-py3

%post
apt-get update &amp;&amp; apt-get -y install wget build-essential 

%runscript
    exec python &quot;$@&quot;
</code></pre>

<p>compile this <em>locally</em> with sudo and singularity.</p>
<pre class="codehilite"><code class="language-bash">sudo singularity build tensorflow.sif tensor.def 
</code></pre>

<p>Create a quick tensorflow test code
tensortest.py</p>
<pre class="codehilite"><code class="language-python">import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
</code></pre>

<p>Copy your files to Rāpoi via sftp (or whatever you prefer)</p>
<pre class="codehilite"><code class="language-bash">sftp &lt;username&gt;@raapoi.vuw.ac.nz
cd &lt;where you want to work&gt;
put *   #put all files in your local directory onto Rāpoi
</code></pre>

<p>Lets quickly test the code via an interactive session on a node.  Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further.</p>
<pre class="codehilite"><code class="language-bash">srun --partition=&quot;parallel&quot; --constraint=&quot;Intel&quot; --pty bash

#now on the remote node - note you might need to wait if nodes are busy
module load singularity #load singularity
singularity shell tensorflow.sif 

#now inside the tensorflow container on the remote node
python tensortest.py 

#once that runs, exit the container
exit #exit the container
exit #exit the interactive session on the node
</code></pre>

<p>Create a submit script using singularity on the cluster</p>
<p>singularity_submit.sh</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=singularity_test
#SBATCH -o sing_test.out
#SBATCH -e sing_test.err
#SBATCH --time=00:10:00
#SBATCH --partition=parallel
#SBATCH --constraint=Intel
#SBATCH --ntasks=1
#SBATCH --mem=4G

module load singularity

#run the container with the runscript defined when we created it
singularity run tensorflow.sif tensortest.py 
</code></pre>

<h3 id="singularitymaxbin2-example">Singularity/MaxBin2 Example<a class="headerlink" href="#singularitymaxbin2-example" title="Permanent link">&para;</a></h3>
<p>In a sensible location, either in your home directory or on the scratch:</p>
<p>Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site.</p>
<pre class="codehilite"><code class="language-bash">module load module load singularity
singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0
mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience
</code></pre>

<p>Download some test data</p>
<pre class="codehilite"><code class="language-bash">mkdir rawdata
curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold &gt; rawdata/20x.scaffold
curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund &gt; rawdata/20x.abund
</code></pre>

<p>Create an output data location</p>
<pre class="codehilite"><code class="language-bash">mkdir output
</code></pre>

<p>Create a submit script using singularity on the cluster</p>
<p>singularity_submit.sh</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=maxbin2_test
#SBATCH -o sing_test.out
#SBATCH -e sing_test.err
#SBATCH --time=00:10:00
#SBATCH --partition=parallel
#SBATCH --ntasks=4
#SBATCH --mem=4G

module load singularity

singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4 
</code></pre>

<h3 id="singularitysandbox-example">Singularity/Sandbox Example<a class="headerlink" href="#singularitysandbox-example" title="Permanent link">&para;</a></h3>
<p>This lets you have root inside a container <em>locally</em> and make changes to it.  This is really handy for determining how to setuop your container.  While you can convert the sandbox container to one you can run on Rāpoi, I suggest you <em>don't do this</em>. Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a <code>.def</code> file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others.</p>
<p>example.def</p>
<pre class="codehilite"><code class="language-bash">BootStrap: library
From: ubuntu:16.04

%post
apt-get update &amp;&amp; apt-get -y install wget build-essential 

%runscript
    exec echo &quot;$@&quot;

%labels
    Author Andre
</code></pre>

<p>Compile this <em>locally</em> with sudo and singularity.  We are using the sandbox flag to create a writable <em>container directory</em> (<code>example/</code>) on our local machine where we have sudo rights.</p>
<pre class="codehilite"><code class="language-bash">sudo singularity build --sandbox example/ example.def
</code></pre>

<p>Now we can run the container we just built, but with sudo rights inside the container.  Your rights outside the container match the rights inside the container, so we need to do this with sudo.</p>
<pre class="codehilite"><code class="language-bash">sudo singularity shell --writable example/
</code></pre>

<p>Inside the container we now have root and can install packages and modify files in the root directories</p>
<pre class="codehilite"><code class="language-bash">Singularity example:~&gt;  apt update
Singularity example:~&gt;  apt install sqlite
Singularity example:~&gt;  touch /test.txt  #create an empty file in root
Singularity example:~&gt;  ls /
Singularity example:~&gt;  exit   #exit container
</code></pre>

<p>To run the container on Rāpoi we convert it to the default immutable image with build.  We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file.</p>
<pre class="codehilite"><code class="language-bash">sudo singularity build new-example-sif example/
</code></pre>

<p>You could now copy the <code>new-example-sif</code> file to Rāpoi and run it there.  However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install.  Once you've done that, I suggest starting afresh and putting <em>everything in the.def file</em>.  That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built.</p>
<h3 id="singularitycustom-conda-container-idba-example">Singularity/Custom Conda Container - idba example<a class="headerlink" href="#singularitycustom-conda-container-idba-example" title="Permanent link">&para;</a></h3>
<p>In this example we'll build a singularity container using conda.  The example is building a container for idba - a genome assembler.  Idba is available in bioconda, but not as a biocontainer.  We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly.</p>
<h4 id="locally">Locally<a class="headerlink" href="#locally" title="Permanent link">&para;</a></h4>
<p>Make sure you have conda setup on your local machine, anaconda and miniconda are good choices.  Create a new conda environment and install idba</p>
<pre class="codehilite"><code class="language-bash">conda create --name idba
conda install -c bioconda idba
</code></pre>

<p>Export your conda environment, we will use this to build the container.</p>
<pre class="codehilite"><code class="language-bash">conda env export &gt; environment.yml
</code></pre>

<p>We will use a singularity definition, basing our build on a docker miniconda image.  There is a bunch of stuff in this file to make sure the conda environment is in the path. <em><a href="https://stackoverflow.com/questions/54678805/containerize-a-conda-environment-in-a-singularity-container">From stackoverflow</a></em></p>
<p><em>idba.def</em></p>
<pre class="codehilite"><code>Bootstrap: docker

From: continuumio/miniconda3

%files
    environment.yml

%environment
    PATH=/opt/conda/envs/$(head -1 environment.yml | cut -d' ' -f2)/bin:$PATH

%post
    echo &quot;. /opt/conda/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.bashrc
    echo &quot;source activate $(head -1 environment.yml | cut -d' ' -f2)&quot; &gt; ~/.bashrc
    /opt/conda/bin/conda env create -f environment.yml

%runscript
    exec &quot;$@&quot;
</code></pre>

<p>Build the image</p>
<pre class="codehilite"><code class="language-bash">sudo singularity build idba.img idba.def
</code></pre>

<p>Now copy the idba.img and environment.yml (technically the environment file is not needed, but not having it creates a warning) to somewhere sensible on Rāpoi.</p>
<h4 id="on-rapoi">On Rāpoi<a class="headerlink" href="#on-rapoi" title="Permanent link">&para;</a></h4>
<p>Create a data directory, so we can separate our inputs and outputs.  Download a paired end illumina read of Ecoli from S3 with wget.  The data comes from the <a href="https://www.illumina.com/informatics/sequencing-data-analysis/data-examples.html">Illumina public data library</a></p>
<pre class="codehilite"><code>mkdir data
cd data 
wget --content-disposition goo.gl/JDJTaz #sequence data
wget --content-disposition goo.gl/tt9fsn #sequence data
cd ..  #back to our project directory
</code></pre>

<p>The reads we have are paired end fastq files but idba requires a fasta file.  We can use a tool built into our container to convert them.  We'll do this on the Rāpoi login node as it is a fast task that doesn't need many resources.</p>
<pre class="codehilite"><code class="language-bash">module load singularity
singularity exec fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa
</code></pre>

<p>Create our sbatch submission script.  Note that this sequence doesn't need a lot of memory, so we'll use 1G. To see your usage after the job has run use <code>vuw-job-report &lt;job-id&gt;</code></p>
<p><em>idba_submit.sh</em></p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=idba_test
#SBATCH -o output.out
#SBATCH -e output.err
#SBATCH --time=00:10:00
#SBATCH --partition=quicktest
#SBATCH --ntasks=12
#SBATCH --mem=1G

module load singularity

singularity exec idba.img idba idba_ud -r data/read.fa -o output
</code></pre>

<p>Now we can submit our script to the queue with</p>
<pre class="codehilite"><code class="language-bash">sbatch idba_submit.sh 
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../notebooks/" class="btn btn-neutral float-left" title="Using Jupyter Notebooks"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../training/" class="btn btn-neutral float-right" title="Training">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../notebooks/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../training/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
