<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://vuw-research-computing.github.io/raapoi-docs/training/gpu-neural-style/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Gpu neural style - R훮poi Cluster Documentation</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="../../extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Gpu neural style";
        var mkdocs_page_input_path = "training/gpu-neural-style.md";
        var mkdocs_page_url = "/raapoi-docs/training/gpu-neural-style/";
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> R훮poi Cluster Documentation
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Overview</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Documentation</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../accessing_the_cluster/">Accessing the Cluster</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../basic_commands/">Basic Commands</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../storage/">Storage and Quotas</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../partitions/">Using Partitions</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../environment/">Preparing your Environment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../running_jobs/">Running Jobs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../parallel_processing/">Parallel Processing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../managing_jobs/">Managing Jobs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cloud_providers/">Connecting to Cloud Providers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../containers/">Using Containers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../notebooks/">Using Jupyter Notebooks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/">Examples</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../">Training</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../usersub/">User Submitted Docs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../support/">Support</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">R훮poi Cluster Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" alt="Docs"></a> &raquo;</li><li>Gpu neural style</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="gpu-example-with-neural-style-in-pytorch">GPU example with neural style in pytorch<a class="headerlink" href="#gpu-example-with-neural-style-in-pytorch" title="Permanent link">&para;</a></h2>
<p>We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages.</p>
<p>The code we use will come from the pytorch example git repo.</p>
<h3 id="clone-the-pytorch-example-repo">Clone the pytorch example repo<a class="headerlink" href="#clone-the-pytorch-example-repo" title="Permanent link">&para;</a></h3>
<p>In a sensible location, clone the rep.</p>
<pre class="codehilite"><code class="language-bash">git clone https://github.com/pytorch/examples.git
cd examples/fast_neural_style  # change to the example we will be running.
</code></pre>

<h3 id="load-the-modules">Load the modules<a class="headerlink" href="#load-the-modules" title="Permanent link">&para;</a></h3>
<p>We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system.  At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system.</p>
<pre class="codehilite"><code class="language-bash">module unuse /home/software/tools/modulefiles/  #unuse the old module system
module use /home/software/tools/eb_modulefiles/all/Core #use the new module system
</code></pre>

<pre class="codehilite"><code class="language-bash">module load fosscuda/2020b
module load PyTorch/1.7.1
module load torchvision/0.8.2-PyTorch-1.7.1
module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6
</code></pre>

<h3 id="optional-setup-a-virtualenv">Optional: Setup a virtualenv<a class="headerlink" href="#optional-setup-a-virtualenv" title="Permanent link">&para;</a></h3>
<pre class="codehilite"><code class="language-bash">python3 -m venv env  # create a virtualenv folder called env. Note! This will likely only work with the python version listed above!
source env/bin/activate # activate the virtualenv
</code></pre>

<p>Now that we've activated the virtual environment, we can install any additional packages we need.  In this case we don't need any additional packages.</p>
<h3 id="download-some-images-to-use-as-content-as-well-as-for-training">Download some images to use as content as well as for training.<a class="headerlink" href="#download-some-images-to-use-as-content-as-well-as-for-training" title="Permanent link">&para;</a></h3>
<p>In your <code>examples/fast_neural_style/</code> directory.</p>
<pre class="codehilite"><code class="language-bash"># Download an image of an octopus to images/content-images. 
## CC BY-SA 3.0 H. Zell
wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ 

# Download an image of The Great Wave off Kanagawa - public domain
wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg
</code></pre>

<p>Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory.  On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy.  Thankfully the GPUs on R훮poi are A100's with 40GB of ram, so we can skip this step.</p>
<p>We will also need to download the pre-trained models for our initial inference runs.</p>
<pre class="codehilite"><code class="language-bash">python download_saved_models.py
</code></pre>

<h3 id="style-some-images-inference">Style some images - inference<a class="headerlink" href="#style-some-images-inference" title="Permanent link">&para;</a></h3>
<p>We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. </p>
<p>submit_cpu.sh</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=pytorch_test
#SBATCH -o _test.out
#SBATCH -e _test.err
#SBATCH --time=00:15:00
#SBATCH --partition=parallel
#SBATCH --ntasks=12
#SBATCH --mem=6G

module unuse /home/software/tools/modulefiles/  #unuse the old module system
module use /home/software/tools/eb_modulefiles/all/Core #use the new module system
module load fosscuda/2020b
module load PyTorch/1.7.1
module load torchvision/0.8.2-PyTorch-1.7.1

#Optional
source env/bin/activate  #activate the virtualenv

# Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg
#
python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG  --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0
</code></pre>

<p>You can check how long the job took to run with <code>vuw-job-history</code>.  The last lines are your last run job, in my case:</p>
<pre class="codehilite"><code class="language-bash">332281        COMPLETED pytorch_t+              00:02:36 
332281.batch  COMPLETED      batch      0.15G   00:02:36 
332281.exte+  COMPLETED     extern      0.15G   00:02:36
</code></pre>

<p>the job took 2:36.</p>
<p>Let's run the inference job again on GPU to see the speedup.</p>
<p>submit_gpu.sh</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=pytorch_test
#SBATCH -o _test.out
#SBATCH -e _test.err
#SBATCH --time=00:15:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --ntasks=2
#SBATCH --mem=60G

module unuse /home/software/tools/modulefiles/  #unuse the old module system
module use /home/software/tools/eb_modulefiles/all/Core #use the new module system
module load fosscuda/2020b
module load PyTorch/1.7.1
module load torchvision/0.8.2-PyTorch-1.7.1

#optional
source env/bin/activate  #activate the virtualenv

# Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg
#
python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG  --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1
</code></pre>

<p>In this case <code>vuw-job-history</code> the job took:</p>
<pre class="codehilite"><code class="language-bash">692973        COMPLETED pytorch_t+              00:00:16 
692973.batch  COMPLETED      batch      0.15G   00:00:16 
692973.exte+  COMPLETED     extern      0.15G   00:00:16 
</code></pre>

<p>but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running <code>vuw-job-report 332320</code> shows:</p>
<pre class="codehilite"><code class="language-bash">Nodes: 1
Cores per node: 2
CPU Utilized: 00:00:07
CPU Efficiency: 43.75% of 00:00:16 core-walltime
Job Wall-clock time: 00:00:08
Memory Utilized: 1.38 MB
Memory Efficiency: 0.00% of 60.00 GB
</code></pre>

<p>The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that <code>vuw-job-report</code> doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory.  The A100's have 40G of ram.</p>
<h3 id="train-a-new-style-computationally-expensive">Train a new style - computationally expensive.<a class="headerlink" href="#train-a-new-style-computationally-expensive" title="Permanent link">&para;</a></h3>
<p>Training a new image style is where we will get the greatest speedup using a GPU.</p>
<p>We will use 13G of training images - <a href="http://cocodataset.org/#download">COCO 2014 Training images dataset </a>. These images have already been downloaded and are accessable at <code>/nfs/home/training/neural_style_data/train2014/</code>.  Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=pytorch_test
#SBATCH -o _test.out
#SBATCH -e _test.err
#SBATCH --time=03:00:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --ntasks=2
#SBATCH --mem=60G

module unuse /home/software/tools/modulefiles/  #unuse the old module system
module use /home/software/tools/eb_modulefiles/all/Core #use the new module system
module load fosscuda/2020b
module load PyTorch/1.7.1
module load torchvision/0.8.2-PyTorch-1.7.1

#Optional
source env/bin/activate  #activate the virtualenv

# Run our job --cuda 1 means run on the GPU                                   
# style-weight and content-weight are just parameters adjusted to give better results
python neural_style/neural_style.py train \
        --dataset /nfs/home/training/neural_style_data/ \
        --style-image images/style-images/wave.jpg \
        --save-model-dir saved_models/style5e10_content_5e4 \
        --style-weight 5e10 \
        --content-weight 5e4 \
        --epochs 2 \
        --cuda 1
</code></pre>

<p>This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory.  Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.</p>
<h3 id="use-our-newly-trained-network">Use our newly trained network<a class="headerlink" href="#use-our-newly-trained-network" title="Permanent link">&para;</a></h3>
<p>submit_gpu.sh</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=pytorch_test
#SBATCH -o _test.out
#SBATCH -e _test.err
#SBATCH --time=00:15:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --ntasks=2
#SBATCH --mem=60G

module unuse /home/software/tools/modulefiles/  #unuse the old module system
module use /home/software/tools/eb_modulefiles/all/Core #use the new module system
module load fosscuda/2020b
module load PyTorch/1.7.1
module load torchvision/0.8.2-PyTorch-1.7.1

#Optional
source env/bin/activate  #activate the virtualenv

# Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg
#
python neural_style/neural_style.py eval \
    --content-image images/content-images/Octopus_vulgaris_02.JPG  \
    --model saved_models/style5e10_content_5e4 \
    --output-image ./test3.jpg --cuda 1
</code></pre>

<h3 id="bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters">Bonus content use a slurm task-array to find the optimum parameters.<a class="headerlink" href="#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters" title="Permanent link">&para;</a></h3>
<p>In the above example we use parameters for style-weight and content-weight.  There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values.   Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays.  Running this example will consume the whole GPU partition for about 12 hours.</p>
<p>First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself.</p>
<p>In the parameter list, the first column is style-weight parameters and the second is content-weight parameters.
paramlist.txt</p>
<pre class="codehilite"><code class="language-bash">5e10 1e3
5e10 1e4
5e10 5e4
1e11 1e3
1e11 1e4
1e11 5e4
5e11 1e3
5e11 1e4
5e11 5e4
1e12 1e3
1e12 1e4
1e12 5e4
</code></pre>

<p>In our submission script we will parse these values with <code>awk</code>.  Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. <a href="https://www.digitalocean.com/community/tutorials/how-to-use-the-awk-language-to-manipulate-text-in-linux">Digital ocean has a nice primer on Awk</a></p>
<p>submit_gpu_train_array</p>
<pre class="codehilite"><code class="language-bash">#!/bin/bash

#SBATCH --job-name=pytorch_test
#SBATCH -o _test.out
#SBATCH -e _test.err
#SBATCH --time=10:00:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --ntasks=10
#SBATCH --mem=60G
#SBATCH --array=1-13

module unuse /home/software/tools/modulefiles/  #unuse the old module system
module use /home/software/tools/eb_modulefiles/all/Core #use the new module system
module load fosscuda/2020b
module load PyTorch/1.7.1
module load torchvision/0.8.2-PyTorch-1.7.1

#Optional
source env/bin/activate  #activate the virtualenv

# Run our job --cuda 1 means run on the GPU                                   
#
#awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; 'NR == var {print $1}' paramlist.txt 
style_weight=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; 'NR == var {print $1}' paramlist.txt)
content_weight=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; 'NR == var {print $2}' paramlist.txt)

echo $style_weight
echo $content_weight
python neural_style/neural_style.py train \
    --dataset nfs/home/training/neural_style_data/ \
    --style-image images/style-images/wave.jpg \
    --save-model-dir saved_models/test_params2_epoch2/style${style_weight}_content${content_weight} \
    --style-weight $style_weight \
    --content-weight $content_weight \
    --epochs 2 \
    --cuda 1
</code></pre>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
